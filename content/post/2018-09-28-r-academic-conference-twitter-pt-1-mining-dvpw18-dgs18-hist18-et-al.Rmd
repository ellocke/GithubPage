---
title: '[R] German Academic Twitter, Pt. 1: Mining #dvpw18, #dgs18, #hist18, #informatik2018 et al.'
author: Ilja / fubits
date: '2018-09-28'
categories:
  - Data Mining
  - Rstats
tags:
  - rtweet
  - Twitter
slug: r-academic-conference-twitter-pt-1-mining-dvpw18-dgs18-hist18-et-al
output:
  blogdown::html_page:
    number_sections: yes
    toc: yes
lastmod: '2018-09-28T20:17:07+02:00'
description: In September, four big academic societies in Germany had their annual meeting - all at the same time! You can **not not** harvest their tweets...
thumbnail: /img/thumbs/conference_tweets.jpg
rmdlink: TRUE # Optional
comment: no
autoCollapseToc: no
postMetaInFooter: no
hiddenFromHomePage: no
contentCopyright: no
reward: no
mathjax: no
mathjaxEnableSingleDollar: no
mathjaxEnableAutoNumber: no
hideHeaderAndFooter: no
flowchartDiagrams:
  enable: no
  options: ''
sequenceDiagrams:
  enable: no
  options: ''
---
![](/img/Twitter_conf/Twitter_conf.png "This is what ~~4~~ 5 German academic conferences look like on Twitter"){width=100%}

> Update: Since the conferences are over but there's still some Twitter activity, Tweets posted after 29.09.2018 have been filtered out from the samples.

>Update 2: The Media Studies conference (\#gfm2018) has been included

As (bad) luck has it, ~~four~~ five big academic societies in Germany somehow decided to hold their respective annual meetings within the same week:

  * [Deutsche Vereinigung für Politikwissenschaft / Political Science](https://www.dvpw.de/kongresse/dvpw-kongresse/dvpw2018/){target="_blank"} (\#dvpw18, \#dvpw2018, \#dvpw)
    
  * [Deutsche Gesellschaft für Soziologie / Sociology](https://kongress2018.soziologie.de/aktuelles/){target="_blank"} (\#dgs18, \#dgs2018)
    
  * [Verband der Historiker und Historikerinnen Deutschlands / History](https://www.historikertag.de/Muenster2018/){target="_blank"} (\#histag18 / \#histag2018 / \#historikertag2018)
    
  * [Gesellschaft für Informatik e.V. / Computer Science](https://informatik2018.gi.de/){target="_blank"} (\#informatik2018)
  * [Gesellschaft für Medienwissenschaft / Media Studies](https://gfmedienwissenschaft.de/jahrestagung){target="_blank"} (\#gfm2018)
        
Even though Germany is still a bit behind with regards to Twitter, ~~four~~ five conferences = ~~4x~~ 5x the chance to work on your Twitter mining and text wrangling skills ;). Plus, we get some interesting data for the future practice of our NLP / text processing and social network analysis skills...

So let's just get started with mining. We will use [Mike Kearney's](https://twitter.com/kearneymw){target="_blank"} superb `rtweet` ([package](https://rtweet.info/){target="_blank"}).

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(rtweet)
```

# Preparation
## Setting up `rtweet`

**Get the Token**

Follow the instructions [here](https://rtweet.info/#api-authorization){target="_blank"}, set up your Twitter app and save your token. 

You'll get something like this (caution: fake credentials)
```{r eval=FALSE}
appname <- "your_app_name"
key <- "your_consumer_key"
secret <- "your_seceret"
```

**Register your App with R.**
```{r eval=FALSE}
twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret)
```

And save your token in your environment / home path / working directory.

**Save token in Root dir / Home path**
```{r eval=FALSE}
## path of home directory
home_directory <- path.expand("~/R")
file_name <- file.path(home_directory, "twitter_token.rds")

## save token to home directory
saveRDS(twitter_token, file = file_name)
# saveRDS(twitter_token, "twitter_token.rds") # save locally in wd
twitter_token <- readRDS(str_c(home_directory,"/twitter_token.rds"))

```

**Token check**
```{r eval=FALSE}
identical(twitter_token, get_token())
#> TRUE
```

## `getTimeString()` Helper Function
I will use this function for saving time-stamped samples of Tweets
```{r}
getTimeString <- function() {
  Sys.time() %>% str_extract_all(regex("[0-9]")) %>%
    unlist() %>% glue::glue_collapse()
  }
getTimeString()
```

## (Prepare filepath for .rds with `here()`)
```{r}
# library(here) # https://blogdown-demo.rbind.io/2018/02/27/r-file-paths/
# blogdown-specific work-around for the `data`-folder
data_path <- here("static", "data", "ConferenceTweets", "/")
if (!dir.exists(data_path)) dir.create(data_path)
# saveRDS(mtcars, str_c(data_path, "test", ".rds")) # test filepath
# readRDS(str_c(data_path, "test", ".rds")) # test filepath
```

# Mining Tweets with `search_tweets()`
## Political Science: \#dvpw18 / \#dvpw2018 (and \#dvpw)

We probably won't get all the tweets with a single request, so what we are going to do is, to request the Tweets multiple times, consolidate the requests, and finally extract unique Tweets with `dplyr::distinct()` to get a pretty good sample.

Notice, that we can request `recent` and `mixed` samples (`popular` doesn't seem to work for me.)

### Mining

The workflow suggested here is that you mine a couple of samples (or mine new samples hours or days later), save these sample with time-stamped and therefore unique file names (as `.rds`), and than consolidate and extract unique tweets with `dplyr::distinct()`

```{r cache=TRUE, eval=FALSE}
dvpw_tweets <- search_tweets(q = "#dvpw18 OR #dvpw2018 OR #dvpw", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent (popular)

saveRDS(dvpw_tweets, file =
          str_c(data_path,"dvpw_tweets_", getTimeString(),".rds"))
```

### Wrangling
Here we'll get a file list of all `dvpw_*.rds` files, then `map_dfr()` them to a `data_frame` and finally extract unique Tweets with `distinct()`

```{r cache=TRUE}
## this is just a bit complicated because I'm using an external data folder for blogdown. If you work locally, you can just use:
# map_dfr(dir(path = ".", "dvpw_"), readRDS)

dvpw_rds <- dir(path = data_path, pattern = "dvpw_") %>% 
  str_c(data_path, .) %>% 
  map_dfr(readRDS)
dvpw_collection <- dvpw_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at)
```

> As you can see from `filter(created_at < "2018-09-30")` we will only consider tweets posted before Sunday, 30.09.2018 (for the sake of comparison)

(How to check the latest/earliest Tweet)
```{r eval=FALSE}
min(dvpw_collection$status_id) # https://twitter.com/statuses/1041748634486931465
Tweet <- max(dvpw_collection$status_id) 
browseURL(str_c("https://twitter.com/statuses/", Tweet))
```

Time-String for Plotting
```{r cache=TRUE, eval=TRUE}
timeString <- str_c(lubridate::hour(Sys.time()), ":", lubridate::minute(Sys.time()))
```

### Treemap: #dvpw / #dvpw18 / #dvpw2018

We'll need the `treemapify` [package](https://github.com/wilkox/treemapify){target="_blank"} for this.
```{r cache=TRUE}
dvpw_n_tweets <- nrow(dvpw_collection)
dvpw_n_accounts <- length(unique(dvpw_collection$screen_name))
# tidy/dplyr: distinct(screen_name) %>% count()

dvpw_collection %>% 
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018",
       subtitle = paste0("(n = ", dvpw_n_tweets,
                         " Tweets von m = ", dvpw_n_accounts,
                         " Accounts; Stand: 29.09.18, ",
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

### Scatterplot: # of Tweets / RTs / Favs per User
For the scatterplot we'll have to group the single Tweets by user (`$screen_name`), summarise the counts for Tweets, RTs, and Favs, and assign a "discipline" category for later use.
```{r cache=TRUE}
dvpw_counts <- dvpw_collection %>%
  group_by(screen_name) %>%
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "PolSci") %>% 
  arrange(desc(Tweets)) # %>% 
  # top_n(n = 50, wt = tweets) 
```

Scatterplot 
```{r cache=TRUE, fig.width=12}
ggplot(dvpw_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  ggrepel::geom_text_repel(data = dvpw_counts[1:2,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  # scale_size_continuous(breaks = c(50, 100, 150, 200, 250, 300)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018: Retweets & Favs",
       subtitle = paste0("(n = ", dvpw_n_tweets,
                       " Tweets von m = ", dvpw_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

The official society accounts have been quite busy! Well done, [\@dvpw/\@dvpwkongress](https://twitter.com/dvpwkongress/status/1040501085067247616){target="_blank"}, the idea of a Twitter `#TeamTakeOver` worked out rather well! <- **Note to my future self.**)

### Scatterplot without @dvpw and with labels for the top 20
Here we'll need `ggrepel` for non-overlapping labeling. Since the official @dvpw account has been quite an "outlier", let's have an undisturbed look at the rest of the field without @dvpw.

```{r cache=TRUE, fig.width=12}
dvpw_counts %>% filter(screen_name != "dvpw") %>%
  ggplot(aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = dvpw_counts[2:21,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018: Top 20 Accounts (ohne @dvpw)",
         subtitle = paste0("(n = ", sum(filter(dvpw_counts,
                                               screen_name != "dvpw")$Tweets),
                           " Tweets von m = ", dvpw_n_accounts - 1,
                         " Accounts, ohne @dvpw; Top 20 Label, Stand: 29.09.18, ",
                         "23:59"," Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

### (TODO: Creating Twitter Lists)
>tba, but we could automate creating of user lists from hashtags for conferences... This might be useful for live-curating Twitter handles for better credits to speakers.

```{r eval=FALSE}
# we need a plain character vector here
dvpw_nicks <- dvpw_collection %>% distinct(screen_name) %>% unlist()
post_list(dvpw_nicks[1:100], name = "dvpw2018", private = TRUE, destroy = FALSE)
#> Can only add 100 users at a time. Adding users[1:100]...
list_length <- length(dvpw_nicks)
post_list(dvpw_nicks[101:200], slug = "dvpw2018", private = TRUE, destroy = FALSE)
post_list(dvpw_nicks[200:length(dvpw_nicks)], slug = "dvpw2018", private = TRUE, destroy = FALSE)

# delete with
# post_list(slug = "dvpw2018", destroy = TRUE)
```


## Sociology: #dgs18 / #dgs2018

Let's have a look at how German Sociologists performed on Twitter. Like above, I've mined the Tweets multiple times in order to get a good sample.

**Mine**
```{r cache=TRUE, eval=FALSE}
dgs_tweets <- search_tweets(q = "#dgs18 OR #dgs2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "mixed") # mixed recent (popular)

saveRDS(dgs_tweets,
        file =  str_c(data_path,"dgs_tweets_", getTimeString(),".rds")) 
```

**Wrangle**
```{r cache=TRUE}
dgs_rds <- dir(path = data_path, pattern = "dgs_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
dgs_collection <- dgs_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Sociology: Treemap**
```{r cache=TRUE}
dgs_n_tweets <- nrow(dgs_collection)
dgs_n_accounts <- length(unique(dgs_collection$screen_name))

dgs_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #dgs18 / #dgs2018",
       subtitle = str_c("(n = ", dgs_n_tweets,
                         " Tweets von m = ", dgs_n_accounts,
                         " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```
That looks rather different from the #dvpw2018 community. Less institutional dominance and actually, many more individual Twitter users (`r dgs_n_accounts` active users vs `r dvpw_n_accounts` in team PolSci).

**Sociology: per-User**
```{r cache=TRUE}
dgs_counts <- dgs_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>%
  mutate(discipline = "Sociology") %>% 
  arrange(desc(Tweets)) # %>%
  # top_n(n = 50, wt = tweets) 
```

```{r cache=TRUE, fig.width=12}
ggplot(dgs_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  scale_x_continuous(breaks = c(10, 20, 30, 40, 50, 100, 150, 175)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #dgs18 / #dgs2018: Retweets & Favs",
       subtitle = paste0("(n = ", dgs_n_tweets,
                       " Tweets von m = ", dgs_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```
Wow, that is quite different, right? Less individual tweets per user, less retweets, but a significantly higher Fav rate. Interesting. Shall we assume that Sociologist are more introvert and maybe have more empathy for others? :)

**Sociology: Top 20 labelled**
```{r cache=TRUE, fig.width=12}
ggplot(dgs_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = dgs_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(10, 20, 30, 40, 50, 100, 150, 175)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #dgs18 / #dgs2018: Top 20 Accounts",
         subtitle = paste0("(n = ", dgs_n_tweets,
                         " Tweets von m = ", dgs_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ", 
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Historians: #histag18 / #histag2018 / #historikertag2018
Next, let's look have at the Twitter activity of German History scholars.

**Mine**
```{r cache=TRUE, eval=FALSE}
hist_tweets <- search_tweets(q = "#histag18 OR #histag2018 OR #historikertag2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "mixed") # mixed recent popular

saveRDS(hist_tweets, file =
          str_c(data_path,"hist_tweets_",getTimeString(),".rds"))
```

**Wrangle**
```{r cache=TRUE}
hist_rds <- dir(path = data_path, pattern = "hist_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
hist_collection <- hist_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Historians: Treemap**
```{r cache=TRUE}
hist_n_tweets <- nrow(hist_collection)
hist_n_accounts <- length(unique(hist_collection$screen_name))

hist_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018",
       subtitle = paste0("(n = ", hist_n_tweets,
                         " Tweets von m = ", hist_n_accounts,
                         " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

**Historians: per-User**
```{r}
hist_counts <- hist_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>%
  mutate(discipline = "History") %>% 
  arrange(desc(Tweets)) # %>%
  # top_n(n = 50, wt = tweets)  
```

```{r cache=TRUE, fig.width=12}
ggplot(hist_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  # scale_size_continuous(breaks = c(50, 100, 150, 200, 250, 300)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018: Retweets & Favs",
       subtitle = paste0("(n = ", hist_n_tweets,
                       " Tweets von m = ", hist_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

**Historians: Top 20 labelled**
```{r cache=TRUE, fig.width=12}
ggplot(hist_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = hist_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018: Top 20 Accounts",
         subtitle = paste0("(n = ", hist_n_tweets,
                         " Tweets von m = ", hist_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Computer Science: \#informatik2018

(Of course, CS scholars are rather disciplined and stick to one hashtag :)
\#informatik18 has only 3 Tweets so far, and \#informatiktage only 2 users...)

**Mine**
```{r cache=TRUE, eval=FALSE}
inf_tweets <- search_tweets(q = "#informatik2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent popular

saveRDS(inf_tweets, file =
          str_c(data_path,"inf_tweets_",getTimeString(),".rds"))
```

**Wrangle**
```{r cache=TRUE}
inf_rds <- dir(path = data_path, pattern = "inf_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
inf_collection <- inf_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Treemap**
```{r cache=TRUE}
inf_n_tweets <- nrow(inf_collection)
inf_n_accounts <- length(unique(inf_collection$screen_name))

inf_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #informatik2018",
       subtitle = paste0("(n = ", inf_n_tweets,
                         " Tweets von m = ", inf_n_accounts,
                         " Accounts; Stand: 29.09.18, ", 
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```
Hm, that's quite a few Tweets for a presumably Tech-savie community...

**Scatterplot with per-user activity**
```{r cache=TRUE}
inf_counts <- inf_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "CS") %>% 
  arrange(desc(Tweets))
  # top_n(n = 50, wt = tweets) %>% 
```

```{r cache=TRUE, fig.width=12}
ggplot(inf_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  # scale_size_continuous(breaks = c(50, 100, 150, 200, 250, 300)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #informatik2018: Retweets & Favs",
       subtitle = paste0("(n = ", inf_n_tweets,
                       " Tweets von m = ", inf_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

> So there's some truth in "I'm a Computer Scientist. We don't use Twitter"...

**Scatterplot: Top 20 labelled**
```{r cache=TRUE, fig.width=12}
ggplot(inf_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = inf_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 20, 40, 60, 80)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #informatik2018: Top 20 Accounts",
         subtitle = paste0("(n = ", inf_n_tweets,
                         " Tweets von m = ", inf_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Media Studies: #gfm2018
> As I have just been informed, the German Society for Media Studies als had their annual meeting this week. That's like some weird multidisciplinary but still strictly unidisciplinary academic conspiracy...

Let's have a look at \#gfm2018

**Mine**
```{r cache=TRUE, eval=FALSE}
gfm_tweets <- search_tweets(q = "#gfm2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent popular

saveRDS(gfm_tweets, file =
          str_c(data_path,"gfm_tweets_",getTimeString(),".rds"))
```

**Wrangle**
```{r cache=TRUE}
gfm_rds <- dir(path = data_path, pattern = "gfm_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
gfm_collection <- gfm_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Treemap**
```{r cache=TRUE}
gfm_n_tweets <- nrow(gfm_collection)
gfm_n_accounts <- length(unique(gfm_collection$screen_name))

gfm_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #gfm2018",
       subtitle = paste0("(n = ", gfm_n_tweets,
                         " Tweets von m = ", gfm_n_accounts,
                         " Accounts; Stand: 29.09.18, ", 
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```
> Let's treat this as preliminary. I've just mined the Tweets for the first time, so a couple more samples might another couple of Tweets.

**Scatterplot with per-user activity**
```{r cache=TRUE}
gfm_counts <- gfm_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "MediaStudies") %>% 
  arrange(desc(Tweets))
  # top_n(n = 50, wt = tweets) %>% 
```

Since there's not too much activity for \#gfm2018, we can jump to the labelled scatterplot.

**Scatterplot: Top 20 labelled**
```{r cache=TRUE, fig.width=12}
ggplot(gfm_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = gfm_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 10, 20, 40, 60, 80, 100, 110)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ", gfm_n_tweets,
                         " Tweets von m = ", gfm_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

# Some Comparisons
First, we will need to bind the four tibbles together. First, let's get the total numbers of unique Tweets and unique users:
```{r}
all_cons <- bind_rows(dvpw_collection, dgs_collection, hist_collection, inf_collection, gfm_collection)
all_n_accounts <- all_cons %>% distinct(screen_name) %>% nrow()
all_n_tweets <- all_cons %>% distinct(status_id) %>% nrow()
```
So, this week, `r all_n_accounts` German academic Twitter accounts have been active at four conferences in total, producing `r all_n_tweets` individual Tweets. Actually, that's quite impressive!

Now we can bind the aggregated *_counts.
```{r}
all_cons_per_user <- bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(screen_name) %>% 
  distinct(screen_name, .keep_all = TRUE) %>% 
  mutate(avg_output = (Tweets + RT + Favs)/3) %>% 
  arrange(desc(avg_output)) #> 1262
# all_cons_per_user %>% distinct(screen_name) %>% count() #> 1262
all_cons_per_user %>% head(20)
```

## Joint Scatterplot: per-User
Let's have a look at this week's German academic Twitter crowd as a whole:
```{r cache=TRUE, fig.width=12}
ggplot(all_cons_per_user, aes(x = Favs, y = RT, color = discipline)) +
    geom_point(aes(size = Tweets), alpha = 0.5) +
    ggrepel::geom_text_repel(data = all_cons_per_user[1,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d(option = "D") +
    scale_x_continuous(breaks = c(0, 100, 200, 300, 400)) +
    theme_minimal() +
    guides(colour = guide_legend(override.aes = list(size = 5, stroke = 1.5))) +
    labs(size = "Anzahl Tweets",
         color = "Disziplin",
         title = "Twitter-Aktivität zu Twitter-Aktivität zu #dvpw*18, #dgs*18, #hist*18, #informatik2018 und #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ", all_n_tweets,
                         " Tweets von m = ", all_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Joint Scatterplot: Top 20 labelled (w/o @dvpw)
```{r cache=TRUE, fig.width=12}
all_cons_per_user %>% 
  filter(screen_name != "dvpw") %>%
  ggplot(aes(x = Favs, y = RT, color = discipline)) +
    geom_point(aes(size = Tweets), alpha = 0.5) +
    ggrepel::geom_text_repel(data = all_cons_per_user[2:21,],
                             aes(label = screen_name), alpha = 1) +
    coord_fixed() +
    scale_color_viridis_d(option = "D") +
    scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250)) +
    theme_minimal() +
    guides(colour = guide_legend(override.aes = list(size = 5, stroke = 1.5))) +
    labs(size = "Anzahl Tweets",
         color = "Disziplin",
         title = "Twitter-Aktivität zu #dvpw*18, #dgs*18, #hist*18, #informatik2018 und #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ", all_n_tweets,
                         " Tweets von m = ", all_n_accounts-1,
                         " Accounts (ohne @dvpw); Top 20 Label, Stand: 29.09.18, ","23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```
That's what one week of academic twitter activity in Germany looks like, Duh!

## Boxplots: Overall Distribution of Activities by Discipline
For Boxplot, we need to wrangle the data into long (~tidy) form:
```{r}
dvpw_box <- dvpw_counts %>% 
  gather("Metric", "Total", 2:4) #%>%
  # mutate(Discipline = "PolSci")
dvpw_box %>% filter(screen_name == "dvpw")
```

```{r}
dgs_box <- dgs_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "Socio")
```

```{r}
hist_box <- hist_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "History")
```

```{r}
inf_box <- inf_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "CS")
```
```{r}
gfm_box <- gfm_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "CS")
```

```{r}
bind_rows(dvpw_box, dgs_box, hist_box, inf_box, gfm_box) %>% 
  ggplot() +
  geom_boxplot(aes(fct_inorder(Metric), Total)) +
  scale_x_discrete() +
  scale_fill_viridis_d() +
  facet_wrap(vars(discipline))
```

```{r}
bind_rows(dvpw_box, dgs_box, hist_box, inf_box, gfm_box) %>% 
  ggplot() +
  geom_violin(aes(fct_inorder(Metric), Total, fill = Metric)) +
  scale_x_discrete() +
  scale_fill_viridis_d() +
  facet_wrap(vars(discipline)) +
  labs(x = "Distribution of Tweets / RT / Favs per User",
       legend = NULL) +
  theme_light()
```
Mmmh, I think I should try those [beeswarm-plots](https://github.com/eclarke/ggbeeswarm){target="_blank"} soon-ish here...

# One final table: The overall activity compared by numbers
What, if we simply compare the disciplines' Twitter performance by simple totals?
```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  arrange(desc(Users))
```

And we if we average out Tweets+RTs+Favs per User?
```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  mutate(avg_output = (Tweets + RT + Fav)/Users) %>% 
  arrange(desc(avg_output))
```

And now let's have look without the `#TeamTakeOver` coup by `@dvpw`:
```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  filter(screen_name != "dvpw") %>% 
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  mutate(avg_output = (Tweets + RT + Fav)/Users) %>% 
  arrange(desc(avg_output))
```

I guess it is fair to conclude that well organised Twitter takeovers by conference participants have quite an effect on the visibility of a conference.

Further interpretation is up to you :)
  
# What's next

One hint:
```{r eval=FALSE, warning=FALSE, message=FALSE}
library(quanteda)
library(tidygraph)
```

But that is for another post...
