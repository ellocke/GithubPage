---
title: '[R] German Academic Twitter, Pt. 1: Mining #dvpw18, #dgs18, #hist18, #informatik2018 et al.'
author: Ilja / fubits
date: '2018-09-28'
categories:
  - Data Mining
  - Rstats
tags:
  - rtweet
  - Twitter
slug: r-academic-conference-twitter-pt-1-mining-dvpw18-dgs18-hist18-et-al
output:
  blogdown::html_page:
    number_sections: yes
    toc: yes
lastmod: '2018-09-28T20:17:07+02:00'
description: "Updated: In September, five big academic societies in Germany had their annual meetings - all at the same time! You can **not not** harvest their tweets. I'll explain step-by-step how to mine them with rtweet and how to wrangle the Tweets for a tidy analysis."
abstract: "Updated: In September, five big academic societies in Germany had their annual meetings - all at the same time! You can **not not** harvest their tweets. I'll explain step-by-step how to mine them with rtweet and how to wrangle the Tweets for a tidy analysis."
thumbnail: /img/thumbs/conference_tweets.jpg
rmdlink: TRUE # Optional
comment: no
autoCollapseToc: no
postMetaInFooter: no
hiddenFromHomePage: no
contentCopyright: no
reward: no
mathjax: no
mathjaxEnableSingleDollar: no
mathjaxEnableAutoNumber: no
hideHeaderAndFooter: no
flowchartDiagrams:
  enable: no
  options: ''
sequenceDiagrams:
  enable: no
  options: ''
---
# Setting: The Big 5 on Twitter

![](/img/Twitter_conf/Twitter_conf.png "This is what ~~4~~ 5 German academic conferences look like on Twitter"){width=100%}

>**Update 3**: As I've stumbled upon some irregularities in my [follow-up post](https://ellocke.github.io/post/r-german-academic-twitter-pt-2-from-data-to-corpus-with-a-turkish-twist/#the-turkish-plot-twist), it turned out that the Twitter sample for the Sociology conference (esp. `#dgs2018`) was heavily cross-poluted by another popular event using the same Hashtag in Turkey... This has been adressed in the [follow-up post](https://ellocke.github.io/post/r-german-academic-twitter-pt-2-from-data-to-corpus-with-a-turkish-twist/#the-turkish-plot-twist), ~~but has yet to be implemented here~~ and now is also adjusted for in this [post](#sociology-dgs18-dgs2018-update).

> Update 2: The Media Studies conference (\#gfm2018) has been included

> Update: Since the conferences are over but there's still some Twitter activity, Tweets posted after 29.09.2018 have been filtered out from the samples.

As (bad) luck has it, ~~four~~ five big academic societies in Germany somehow decided to hold their respective annual meetings within the same week:

  * [Deutsche Vereinigung für Politikwissenschaft / Political Science](https://www.dvpw.de/kongresse/dvpw-kongresse/dvpw2018/){target="_blank"} (\#dvpw18, \#dvpw2018, \#dvpw)
    
  * [Deutsche Gesellschaft für Soziologie / Sociology](https://kongress2018.soziologie.de/aktuelles/){target="_blank"} (\#dgs18, \#dgs2018)
    
  * [Verband der Historiker und Historikerinnen Deutschlands / History](https://www.historikertag.de/Muenster2018/){target="_blank"} (\#histag18 / \#histag2018 / \#historikertag2018)
    
  * [Gesellschaft für Informatik e.V. / Computer Science](https://informatik2018.gi.de/){target="_blank"} (\#informatik2018)
  * [Gesellschaft für Medienwissenschaft / Media Studies](https://gfmedienwissenschaft.de/jahrestagung){target="_blank"} (\#gfm2018)
        
Even though Germany is still a bit behind with regards to Twitter, ~~four~~ five conferences = ~~4x~~ 5x the chance to work on your Twitter mining and text wrangling skills ;). Plus, we get some interesting data for the future practice of our NLP / text processing and social network analysis skills...

So let's just get started with mining. We will use [Mike Kearney's](https://twitter.com/kearneymw){target="_blank"} superb `rtweet` ([package](https://rtweet.info/){target="_blank"}).

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(rtweet)
```

# Preparation
## Setting up `rtweet`

**Get the Token**

Follow the instructions [here](https://rtweet.info/#api-authorization){target="_blank"}, set up your Twitter app and save your token. 

You'll get something like this (caution: fake credentials)

```{r eval=FALSE}
appname <- "your_app_name"
key <- "your_consumer_key"
secret <- "your_seceret"
```

**Register your App with R.**

```{r eval=FALSE}
twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret)
```

And save your token in your environment / home path / working directory.

**Save token in Root dir / Home path**

```{r eval=FALSE}
## path of home directory
home_directory <- path.expand("~/R")
file_name <- file.path(home_directory, "twitter_token.rds")

## save token to home directory
saveRDS(twitter_token, file = file_name)
# saveRDS(twitter_token, "twitter_token.rds") # save locally in wd
twitter_token <- readRDS(str_c(home_directory,"/twitter_token.rds"))

```

**Token check**

```{r eval=FALSE}
identical(twitter_token, get_token())
#> TRUE
```

## `getTimeString()` Helper Function

I will use this function for saving time-stamped samples of Tweets

```{r}
getTimeString <- function() {
  Sys.time() %>% str_extract_all(regex("[0-9]")) %>%
    unlist() %>% glue::glue_collapse()
  }
getTimeString()
```

## (Prepare filepath for .rds with `here()`)

```{r}
# library(here) # https://blogdown-demo.rbind.io/2018/02/27/r-file-paths/
# blogdown-specific work-around for the `data`-folder
data_path <- here("static", "data", "ConferenceTweets", "/")
if (!dir.exists(data_path)) dir.create(data_path)
# saveRDS(mtcars, str_c(data_path, "test", ".rds")) # test filepath
# readRDS(str_c(data_path, "test", ".rds")) # test filepath
```

# Mining Tweets with `search_tweets()`

We probably won't get all the tweets with a single request, so what we are going to do is, to request the Tweets multiple times, consolidate the requests, and finally extract unique Tweets with `dplyr::distinct()` to get a pretty good sample.

Notice, that we can request `recent` and `mixed` samples (However, `popular` doesn't seem to work for me, atm.)

## Political Science: \#dvpw18 / \#dvpw2018 (and \#dvpw)

### Mining

The workflow suggested here is that you mine a couple of samples (or mine new samples hours or days later), save these samples with time-stamped and therefore unique file names (as `group_timestamp.rds`), and than consolidate and extract unique tweets with `dplyr::distinct()`

```{r cache=TRUE, eval=FALSE}
dvpw_tweets <- search_tweets(q = "#dvpw18 OR #dvpw2018 OR #dvpw", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent (popular)

saveRDS(dvpw_tweets, file =
          str_c(data_path,"dvpw_tweets_", getTimeString(),".rds"))
```

### Wrangling

Here we'll get a file list of all `dvpw_*.rds` files, then `map_dfr()` them to a `data_frame` and finally extract unique Tweets with `distinct()`

```{r}
## this is just a bit complicated because I'm using an external data folder for blogdown. If you work locally, you can just use:
# map_dfr(dir(path = ".", "dvpw_"), readRDS)

dvpw_rds <- dir(path = data_path, pattern = "dvpw_") %>% 
  str_c(data_path, .) %>% 
  map_dfr(readRDS)
dvpw_collection <- dvpw_rds %>% 
  distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at > "2018-09-23" &
         created_at < "2018-09-30") %>%
  arrange(created_at)
```

> As you can see from `filter(created_at < "2018-09-30")` we will only consider tweets posted before Sunday, 30.09.2018 (for the sake of comparison)

(How to check the latest/earliest Tweet)

```{r eval=FALSE}
min(dvpw_collection$status_id) # https://twitter.com/statuses/1041748634486931465
Tweet <- max(dvpw_collection$status_id) 
browseURL(str_c("https://twitter.com/statuses/", Tweet))
```

Time-String for Plotting

```{r cache=TRUE, eval=TRUE}
timeString <- str_c(lubridate::hour(Sys.time()), ":", lubridate::minute(Sys.time()))
```

### Treemap: #dvpw / #dvpw18 / #dvpw2018

We'll need the `treemapify` [package](https://github.com/wilkox/treemapify){target="_blank"} for this.

```{r}
dvpw_n_tweets <- nrow(dvpw_collection)
dvpw_n_accounts <- length(unique(dvpw_collection$screen_name))
# tidy/dplyr: distinct(screen_name) %>% count()

dvpw_collection %>% 
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018",
       subtitle = paste0("(n = ", dvpw_n_tweets,
                         " Tweets von m = ", dvpw_n_accounts,
                         " Accounts; Stand: 29.09.18, ",
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

### Scatterplot: # of Tweets / RTs / Favs per User

For the scatterplot we'll have to group the single Tweets by user (`$screen_name`), summarise the counts for Tweets, RTs, and Favs, and assign a "discipline" category for later use.

```{r}
dvpw_counts <- dvpw_collection %>%
  group_by(screen_name) %>%
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "PolSci") %>% 
  arrange(desc(Tweets)) # %>% 
  # top_n(n = 50, wt = tweets) 
```

**Scatterplot**

```{r fig.width=12}
ggplot(dvpw_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  ggrepel::geom_text_repel(data = dvpw_counts[1:2,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  scale_x_continuous(breaks = scales::pretty_breaks(6)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018: Retweets & Favs",
       subtitle = paste0("(n = ", dvpw_n_tweets,
                       " Tweets von m = ", dvpw_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

The official society accounts have been quite busy! Well done, [\@dvpw/\@dvpwkongress](https://twitter.com/dvpwkongress/status/1040501085067247616){target="_blank"}, the idea of a Twitter `#TeamTakeOver` worked out rather well! <- **Note to my future self.**)

> To be precise, the collective action of `@dvpw` has produced `r str_c("n = ", dvpw_counts %>% filter(screen_name == "dvpw") %>% select(Tweets))` individual Tweets!

### Scatterplot without @dvpw and with labels for the top 20

Here we'll need `ggrepel` for non-overlapping labelling. As the official @dvpw account has been quite an "outlier", let's have an undisturbed look at the rest of the field without @dvpw.

```{r fig.width=12}
dvpw_counts %>% filter(screen_name != "dvpw") %>%
  ggplot(aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = dvpw_counts[2:21,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = scales::pretty_breaks(6)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #dvpw / #dvpw18 / #dvpw2018: Top 20 Accounts (ohne @dvpw)",
         subtitle = paste0("(n = ",
                           sum(filter(dvpw_counts,
                                      screen_name != "dvpw")$Tweets),
                           " Tweets von m = ", dvpw_n_accounts - 1,
                         " Accounts, ohne @dvpw; Top 20 Label, Stand: 29.09.18, ",
                         "23:59"," Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

### (TODO: Creating Twitter Lists)

>tba, but we could automate creating of user lists from hashtags for conferences... This might be useful for live-curating Twitter handles for better credits to speakers.

```{r eval=FALSE}
# we need a plain character vector here
dvpw_nicks <- dvpw_collection %>% distinct(screen_name) %>% unlist()
post_list(dvpw_nicks[1:100], name = "dvpw2018", private = TRUE, destroy = FALSE)
#> Can only add 100 users at a time. Adding users[1:100]...
list_length <- length(dvpw_nicks)
post_list(dvpw_nicks[101:200], slug = "dvpw2018", private = TRUE, destroy = FALSE)
post_list(dvpw_nicks[200:length(dvpw_nicks)], slug = "dvpw2018", private = TRUE, destroy = FALSE)

# delete with
# post_list(slug = "dvpw2018", destroy = TRUE)
```


## Sociology: #dgs18 / #dgs2018 (UPDATE!)

Let's have a look at how German Sociologists performed on Twitter. Like above, I've mined the Tweets multiple times in order to get a good sample.

**Mine**

```{r cache=TRUE, eval=FALSE}
dgs_tweets <- search_tweets(q = "#dgs18 OR #dgs2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent (popular)

saveRDS(dgs_tweets,
        file =  str_c(data_path,"dgs_tweets_", getTimeString(),".rds")) 
```

**Wrangle**

```{r}
dgs_rds <- dir(path = data_path, pattern = "dgs_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
```

> As I have discovered some irregularities while pre-processing the Tweets for a corpus analysis (see the [follow-up post](https://ellocke.github.io/post/r-german-academic-twitter-pt-2-from-data-to-corpus-with-a-turkish-twist/#the-turkish-plot-twist) for details), the Sociology Twitter sample needs extra-filtering. Consequently, all the following analyses and plots have been re-done and updated.

**As we can see from the following time-series plot, the `#dgs2018` hashtag has, first been used by the Turkish community before German Sociologists took over around the 23th:**

```{r}
dgs_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>% 
  filter(lang == "tr" | lang == "de") %>% 
  group_by(lang) %>% 
  rtweet::ts_plot() +
    theme_minimal()
```

**Therefore, we have to do two things: narrow down the time period to 23.9.-29.9. and filter out as many Turkish accounts as possible.**

> For the sake of comparability, 23.9. has been set as the lower limit for the other disciplines, too.

```{r}
# ID TR users
tr_user <- dgs_rds %>% 
  distinct(status_id, .keep_all = TRUE) %>%
  filter(lang == "tr") %>%
  select(user_id) %>% 
  distinct()

## Syntax for hand-picking suspicious hashtags
# dgs_collection %>%
#   filter(lang == "und") %>% 
#   filter(!str_detect(text,"yks2018|yksdil|dgsankara|cumhuriyetüniversitesi|DanceKafe")) %>% 
#   select(screen_name,text)

# ID lang=="und" Tweets with certain hashtags ("yks2019", "yksdil", ...)
und_user <- dgs_rds %>% 
  distinct(status_id, .keep_all = TRUE) %>%
  filter(str_detect(text,"yks2018|yksdil|dgsankara|
                     cumhuriyetüniversitesi|DanceKafe")) %>% 
  select(user_id) %>% 
  distinct()

# limit time period
dgs_collection <- dgs_rds %>%
  distinct(status_id, .keep_all = TRUE) %>%
  filter(lang != "tr") %>% 
  filter(created_at > "2018-09-23" &
         created_at < "2018-09-30") %>%
  arrange(created_at)
         
#remove TR users from collection
dgs_collection <- dgs_collection %>%
  anti_join(bind_rows(tr_user,und_user), by = "user_id")
```



**Sociology: Treemap**

```{r}
dgs_n_tweets <- nrow(dgs_collection)
dgs_n_accounts <- length(unique(dgs_collection$screen_name))

dgs_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #dgs18 / #dgs2018 (UPDATE)",
       subtitle = str_c("(n = ", dgs_n_tweets,
                         " Tweets von m = ", dgs_n_accounts,
                         " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```
That looks rather different from the #dvpw2018 community. Less institutional dominance and ~~actually, many more~~ **less** individual Twitter users (`r dgs_n_accounts` active users vs `r dvpw_n_accounts` in team PolSci).

**just for comparison, here's what my first bad take looked like:**
```{r echo=FALSE}
dgs_rds %>% distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at < "2018-09-30") %>%
  arrange(created_at) %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "(INVALID)__Twitter-Aktivität zu #dgs18 / #dgs2018__ (INVALID)",
       subtitle = str_c("(n = ", length(unique(dgs_rds$status_id)),
                         " Tweets von m = ",
                        length(unique(dgs_rds$user_id)),
                         " Accounts; Stand: 29.09.18, ", "23:59" ,
                        " Uhr;", " by @fubits)")) +
  guides(fill = FALSE)
```

> So, unfortunately, after removing all the Turkish `#dgs2018` we're down from initially `r length(unique(dgs_rds$status_id))` to **`r dgs_n_tweets` unique Tweets** from **`r dgs_n_accounts` unique Users** (instead of `r length(unique(dgs_rds$user_id))`)... 

**Sociology: per-User**

```{r}
dgs_counts <- dgs_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>%
  mutate(discipline = "Sociology") %>% 
  arrange(desc(Tweets)) # %>%
  # top_n(n = 50, wt = tweets) 
```

```{r fig.width=12}
ggplot(dgs_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  scale_x_continuous(breaks = c(10,20,30,40,50,75,100,150,175)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #dgs18 / #dgs2018: Retweets & Favs (UPDATE)",
       subtitle = paste0("(n = ", dgs_n_tweets,
                       " Tweets von m = ", dgs_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

So, that is quite different from PolSci, right? Less individual tweets per user, less retweets, but a significantly higher Fav rate. Interesting. Shall we assume that Sociologist are more introvert and maybe have more empathy for others? :)

**Sociology: Top 20 labelled**
```{r fig.width=12}
ggplot(dgs_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = dgs_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(10,20,30,40,50,75,100,150,175)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #dgs18 / #dgs2018: Top 20 Accounts (UPDATE)",
         subtitle = paste0("(n = ", dgs_n_tweets,
                         " Tweets von m = ", dgs_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ", 
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Historians: #histag18 / #histag2018 / #historikertag2018
Next, let's look have at the Twitter activity of German History scholars.

**Mine**

```{r cache=TRUE, eval=FALSE}
hist_tweets <- search_tweets(q = "#histag18 OR #histag2018 OR #historikertag2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "mixed") # mixed recent popular

saveRDS(hist_tweets, file =
          str_c(data_path,"hist_tweets_",getTimeString(),".rds"))
```

**Wrangle**

```{r}
hist_rds <- dir(path = data_path, pattern = "hist_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
hist_collection <- hist_rds %>% 
  distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at > "2018-09-23" &
         created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Historians: Treemap**

```{r}
hist_n_tweets <- nrow(hist_collection)
hist_n_accounts <- length(unique(hist_collection$screen_name))

hist_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018",
       subtitle = paste0("(n = ", hist_n_tweets,
                         " Tweets von m = ", hist_n_accounts,
                         " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

**Historians: per-User**

```{r}
hist_counts <- hist_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>%
  mutate(discipline = "History") %>% 
  arrange(desc(Tweets)) # %>%
  # top_n(n = 50, wt = tweets)  
```

```{r fig.width=12}
ggplot(hist_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  scale_x_continuous(breaks = scales::pretty_breaks(6)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018: Retweets & Favs",
       subtitle = paste0("(n = ", hist_n_tweets,
                       " Tweets von m = ", hist_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

**Historians: Top 20 labelled**

```{r fig.width=12}
ggplot(hist_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = hist_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = scales::pretty_breaks(6)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #histag18 / #histag2018 / #historikertag2018: Top 20 Accounts",
         subtitle = paste0("(n = ", hist_n_tweets,
                         " Tweets von m = ", hist_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Computer Science: \#informatik2018

(Of course, CS scholars are rather disciplined and stick to one hashtag :)
\#informatik18 has only 3 Tweets so far, and \#informatiktage only 2 users...)

**Mine**

```{r cache=TRUE, eval=FALSE}
inf_tweets <- search_tweets(q = "#informatik2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent popular

saveRDS(inf_tweets, file =
          str_c(data_path,"inf_tweets_",getTimeString(),".rds"))
```

**Wrangle**

```{r}
inf_rds <- dir(path = data_path, pattern = "inf_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
inf_collection <- inf_rds %>% 
  distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at > "2018-09-23" &
         created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Treemap**

```{r}
inf_n_tweets <- nrow(inf_collection)
inf_n_accounts <- length(unique(inf_collection$screen_name))

inf_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #informatik2018",
       subtitle = paste0("(n = ", inf_n_tweets,
                         " Tweets von m = ", inf_n_accounts,
                         " Accounts; Stand: 29.09.18, ", 
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

Hm, that's quite a few Tweets for a presumably Tech-savie community...

**Scatterplot with per-user activity**

```{r}
inf_counts <- inf_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "CS") %>% 
  arrange(desc(Tweets))
  # top_n(n = 50, wt = tweets) %>% 
```

```{r fig.width=12}
ggplot(inf_counts, aes(x = Favs, y = RT)) +
  geom_point(aes(size = Tweets, color = screen_name)) +
  # ggrepel::geom_text_repel(data = counts[1:10,], aes(label = screen_name)) +
  coord_fixed() +
  scale_color_viridis_d() +
  # scale_size_continuous(breaks = c(50, 100, 150, 200, 250, 300)) +
  guides(color = FALSE) +
  theme_minimal() +
  labs(size = "Anzahl Tweets",
       title = "Twitter-Aktivität zu #informatik2018: Retweets & Favs",
       subtitle = paste0("(n = ", inf_n_tweets,
                       " Tweets von m = ", inf_n_accounts,
                       " Accounts; Stand: 29.09.18, ", "23:59" , " Uhr;"),
       x = "Anzahl Favourites",
       y = "Anzahl Retweets",
       caption = "@fubits")
```

> So there's some truth in "I'm a Computer Scientist. We don't use Twitter"...

**Scatterplot: Top 20 labelled**

```{r fig.width=12}
ggplot(inf_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = inf_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 20, 40, 60, 80)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #informatik2018: Top 20 Accounts",
         subtitle = paste0("(n = ", inf_n_tweets,
                         " Tweets von m = ", inf_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Media Studies: #gfm2018

> As I have [just been informed](https://twitter.com/__evamaria__/status/1046389871001128960){target="_blank"} on Twitter, the German Society for Media Studies also had their annual meeting this week. That's like some weird multidisciplinary but still strictly unidisciplinary academic conspiracy...

Nevertheless, let's have a look at \#gfm2018, too!

**Mine**

```{r cache=TRUE, eval=FALSE}
gfm_tweets <- search_tweets(q = "#gfm2018", # explicit QUERY
      include_rts = FALSE,
      # max_id = ,
      n = 5000,
      verbose = TRUE,
      retryonratelimit = TRUE,
      type = "recent") # mixed recent popular

saveRDS(gfm_tweets, file =
          str_c(data_path,"gfm_tweets_",getTimeString(),".rds"))
```

**Wrangle**

```{r}
gfm_rds <- dir(path = data_path, pattern = "gfm_") %>%
  str_c(data_path, .) %>%
  map_dfr(readRDS)
gfm_collection <- gfm_rds %>%
  distinct(status_id, .keep_all = TRUE) %>%
  filter(created_at > "2018-09-23" &
         created_at < "2018-09-30") %>%
  arrange(created_at)
```

**Treemap**

```{r}
gfm_n_tweets <- nrow(gfm_collection)
gfm_n_accounts <- length(unique(gfm_collection$screen_name))

gfm_collection %>%
  group_by(screen_name) %>%
  summarise(n = n()) %>%
  mutate(share = n / sum(n)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(area = share)) +
    treemapify::geom_treemap(aes(fill = log10(n))) +
    treemapify::geom_treemap_text(
      aes(label = paste0(screen_name, " (", round(share*100,1),"%)"))
      ) +
  scale_fill_viridis_c(direction = -1, option = "C", begin = 0.8) +
  labs(title = "Twitter-Aktivität zu #gfm2018",
       subtitle = paste0("(n = ", gfm_n_tweets,
                         " Tweets von m = ", gfm_n_accounts,
                         " Accounts; Stand: 29.09.18, ", 
                         "23:59" , " Uhr;",
                         " by @fubits)")) +
  guides(fill = FALSE)
```

> Let's treat this as preliminary. I've just mined the Tweets for the first time, so a couple more samples might another couple of Tweets. Don't expect the numbers to double, though!

**Scatterplot with per-user activity**

```{r}
gfm_counts <- gfm_collection %>%
  group_by(screen_name) %>%
  # filter(screen_name != "fubits") %>% 
  summarise(Tweets = n(),
            RT = sum(retweet_count),
            Favs = sum(favorite_count)) %>% 
  mutate(discipline = "MediaStudies") %>% 
  arrange(desc(Tweets))
  # top_n(n = 50, wt = tweets) %>% 
```

Since there's not too much activity for \#gfm2018, we can jump to the labelled scatterplot.

**Scatterplot: Top 20 labelled**

```{r fig.width=12}
ggplot(gfm_counts, aes(x = Favs, y = RT)) +
    geom_point(aes(size = Tweets, color = screen_name), alpha = 0.5) +
    ggrepel::geom_text_repel(data = gfm_counts[1:20,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d() +
    scale_x_continuous(breaks = c(0, 10, 20, 40, 50, 110)) +
    guides(color = FALSE) +
    theme_minimal() +
    labs(size = "Anzahl Tweets",
         title = "Twitter-Aktivität zu #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ", gfm_n_tweets,
                         " Tweets von m = ", gfm_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

# Some Comparisons

Now, we will need to bind the four tibbles together. First, let's get the total numbers of unique Tweets and unique users:

```{r}
all_cons <- bind_rows(dvpw_collection, dgs_collection, hist_collection, inf_collection, gfm_collection)
all_n_accounts <- all_cons %>% distinct(screen_name) %>% nrow()
all_n_tweets <- all_cons %>% distinct(status_id) %>% nrow()
```

So, this week, `r all_n_accounts` German academic Twitter accounts have been active at ~~four~~ five conferences in total, producing `r all_n_tweets` individual Tweets. Actually, that's quite impressive!

Now we can bind the aggregated *_counts.

```{r}
all_cons_per_user <- bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(screen_name) %>% 
  distinct(screen_name, .keep_all = TRUE) %>% 
  mutate(avg_output = (Tweets + RT + Favs)/3) %>% 
  arrange(desc(avg_output)) #> 1262
# all_cons_per_user %>% distinct(screen_name) %>% count() #> 1262
all_cons_per_user %>% head(20) %>% knitr::kable()
```

## Joint Scatterplot: per-User

Let's have a look at this week's German academic Twitter crowd as a whole:

```{r fig.width=12}
ggplot(all_cons_per_user,
       aes(x = Favs, y = RT, color = fct_inorder(discipline))) +
    geom_point(aes(size = Tweets), alpha = 0.5) +
    ggrepel::geom_text_repel(data = all_cons_per_user[1,],
                             aes(label = screen_name)) +
    coord_fixed() +
    scale_color_viridis_d(option = "D") +
    scale_x_continuous(breaks = c(0, 100, 200, 300, 400)) +
    theme_minimal() +
    guides(color = guide_legend(override.aes = list(size = 5,
                                                    stroke = 1.5)
                                )) +
    labs(size = "Anzahl Tweets",
         color = "Disziplin",
         title = "Twitter-Aktivität zu Twitter-Aktivität zu #dvpw*18, #dgs*18, #hist*18, #informatik2018 und #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ", all_n_tweets,
                         " Tweets von m = ", all_n_accounts,
                         " Accounts; Top 20 Label, Stand: 29.09.18, ",
                         "23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

## Joint Scatterplot: Top 20 labelled (w/o @dvpw)

```{r fig.width=12}
all_cons_per_user %>% 
  filter(screen_name != "dvpw") %>%
  ggplot(aes(x = Favs, y = RT, color = fct_inorder(discipline))) +
    geom_point(aes(size = Tweets), alpha = 0.5) +
    ggrepel::geom_text_repel(data = all_cons_per_user[2:21,],
                             aes(label = screen_name), alpha = 1) +
    coord_fixed() +
    scale_color_viridis_d(option = "D") +
    scale_x_continuous(breaks = scales::pretty_breaks()) +
    theme_minimal() +
    guides(colour = guide_legend(override.aes = list(size = 5, 
                                                     stroke = 1.5)
                                 )) +
    labs(size = "Anzahl Tweets",
         color = "Disziplin",
         title = "Twitter-Aktivität zu #dvpw*18, #dgs*18, #hist*18, #informatik2018 und #gfm2018: Top 20 Accounts",
         subtitle = paste0("(n = ",
                           sum(filter(all_cons_per_user,
                                      screen_name != "dvpw")$Tweets),
                         " Tweets von m = ", all_n_accounts-1,
                         " Accounts (ohne @dvpw); Top 20 Label, Stand: 29.09.18, ","23:59", " Uhr)"),
         x = "Anzahl Favourites",
         y = "Anzahl Retweets",
         caption = "@fubits")
```

That's what one week of academic twitter activity in Germany looks like, Duh!

## Boxplots: Overall Distribution of Activities by Discipline

For Boxplot, we need to wrangle the data into long (~tidy) form:

```{r}
dvpw_box <- dvpw_counts %>% 
  gather("Metric", "Total", 2:4) #%>%
  # mutate(Discipline = "PolSci")
dvpw_box %>% filter(screen_name == "dvpw") %>% knitr::kable(format = "html")
```

```{r}
dgs_box <- dgs_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "Socio")

hist_box <- hist_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "History")

inf_box <- inf_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "CS")

gfm_box <- gfm_counts %>% 
  gather("Metric", "Total", 2:4) # %>%
  # mutate(Discipline = "CS")
```

```{r}
bind_rows(dvpw_box, dgs_box, hist_box, inf_box, gfm_box) %>% 
  ggplot() +
  geom_boxplot(aes(fct_inorder(Metric), Total)) +
  scale_x_discrete() +
  scale_fill_viridis_d() +
  facet_wrap(vars(discipline)) +
  theme_light()
```

Hm, maybe Violin Plots can reveal more?

```{r}
bind_rows(dvpw_box, dgs_box, hist_box, inf_box, gfm_box) %>% 
  ggplot() +
  geom_violin(aes(fct_inorder(Metric), Total, fill = Metric)) +
  scale_x_discrete() +
  scale_fill_viridis_d() +
  facet_wrap(vars(discipline)) +
  labs(x = "Distribution of Tweets / RT / Favs per User",
       legend = NULL) +
  theme_light()
```

Mmmh, ok, I think I should try those [beeswarm-plots](https://github.com/eclarke/ggbeeswarm){target="_blank"} soon-ish here...

# Final scores: The overall activity compared by numbers

What if we simply compare the five disciplines' Twitter performance in terms of totals?

```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  arrange(desc(Users))
```

~~Sociology~~ History has the highest number of active users and Favs (wow!), while PolSci has a lead with the total number of Tweets.

And what if we average out Tweets+RTs+Favs per User?

```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  mutate(avg_output = (Tweets + RT + Fav) / Users) %>% 
  arrange(desc(avg_output))
```

Here, the PolSci crowd has been the busiest ~~(and Sociology  was rather lazy)~~. But...

\... let's have look without the `#TeamTakeOver` coup by `@dvpw`:

```{r}
bind_rows(dvpw_counts, dgs_counts, hist_counts, inf_counts, gfm_counts) %>%
  filter(screen_name != "dvpw") %>% 
  group_by(discipline) %>% 
  summarise(Users = n(), Tweets = sum(Tweets),
            RT = sum(RT), Fav = sum(Favs)) %>% 
  mutate(avg_output = (Tweets + RT + Fav) / Users) %>% 
  arrange(desc(avg_output))
```

Well done, Historians! If we ignore @dvpw's Twitter takeover, you actually performed best (in terms of numbers, at least)!

> I guess it is fair to conclude that a well organised Twitter takeover by conference participants can have quite an effect on the visibility of a conference.

Further interpretation is up to you :)
  
# What's next?

One hint:
```{r eval=FALSE, warning=FALSE, message=FALSE}
library(quanteda)
library(tidygraph)
```

But that is for another post...
