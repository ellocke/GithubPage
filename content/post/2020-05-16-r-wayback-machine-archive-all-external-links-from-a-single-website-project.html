---
title: '[R] Wayback Machine - Archive all Links from a Single Website Project'
author: 'Ilja / fubits'
date: '2020-05-16'
slug: r-wayback-machine-archive-all-external-links-from-a-single-website-project
categories:
  - Archiving
  - Open Data
  - Retrieval
  - Rstats
tags:
  - wayback machine
  - API
  - get/post
  - Rcrawler
output:
  blogdown::html_page:
    number_sections: yes
    toc: yes
draft: no
lastmod: '2020-05-16T19:09:00+02:00'
description: "Short proof-of-concept on how to batch-archive a set of URLs with Internet Archive's Wayback Machine & R"
abstract: "Short proof-of-concept on how to batch-archive a set of URLs with Internet Archive's Wayback Machine & R / RCrawler"
thumbnail: '/img/thumbs/wayback-machine-with-R.png'
rmdlink: yes
keywords: []
comment: no
autoCollapseToc: no
postMetaInFooter: no
hiddenFromHomePage: no
mathjax: no
mathjaxEnableSingleDollar: no
mathjaxEnableAutoNumber: no
---


<div id="TOC">
<ul>
<li><a href="#use-case-archive-all-links-from-a-single-web-project"><span class="toc-section-number">1</span> Use Case: Archive All Links from a Single Web Project</a></li>
<li><a href="#theory-crawl---extract---archive"><span class="toc-section-number">2</span> Theory: Crawl - Extract - Archive</a></li>
<li><a href="#minimal-viable-demo---fetchurlstarget-archiveurlsurls"><span class="toc-section-number">3</span> “Minimal Viable Demo” - fetchURLs(target) &amp; archiveURLs(URLs)</a><ul>
<li><a href="#setup"><span class="toc-section-number">3.1</span> Setup</a></li>
<li><a href="#fetchurlsurl---function-to-crawl-scrape-a-single-website"><span class="toc-section-number">3.2</span> fetchURLs(url) - Function to crawl &amp; scrape a single Website</a></li>
<li><a href="#archiveurl---function-to-archive-a-single-url-with-a-post-request"><span class="toc-section-number">3.3</span> archiveURL() - Function to archive a single URL with a POST request</a></li>
<li><a href="#wrapper-function-to-archive-a-vector-of-urls"><span class="toc-section-number">3.4</span> Wrapper function to archive a vector of URLs:</a></li>
</ul></li>
<li><a href="#proof-of-concept---execution"><span class="toc-section-number">4</span> Proof-of-Concept - Execution</a><ul>
<li><a href="#scrape-external-links-from-the-project-website"><span class="toc-section-number">4.1</span> Scrape external Links from the project website</a></li>
<li><a href="#archive-external-links-and-save-as-csv"><span class="toc-section-number">4.2</span> Archive external links and save as CSV</a></li>
</ul></li>
<li><a href="#questionstbd"><span class="toc-section-number">5</span> Questions/TBD</a></li>
</ul>
</div>

<blockquote>
<p>Long time no see, R…. Well, not really, but I’ve been heads down working on some <a href="https://chemicalweapons.gppi.net/" target="_blank">larger projects</a> lately and didn’t have the time to blog. 1 of the projects should be published soon (plenty of hill-shading, maps &amp; data viz), 1 is still in the making (something JS-intense with Google Apps Script &amp; <a href="https://github.com/google/clasp" target="_blank">clasp</a>), but 1 - which is my biggest gig so far - is <a href="(https://chemicalweapons.gppi.net/)%7Btarget=%22_blank%22%7D">online</a> now with all features (incl. my first-ever <a href="https://chemicalweapons.gppi.net/analysis/the-logic/" target="_blank">scrollytelling implementation</a>) and will serve as my use case in this post.</p>
</blockquote>
<div id="use-case-archive-all-links-from-a-single-web-project" class="section level1">
<h1><span class="header-section-number">1</span> Use Case: Archive All Links from a Single Web Project</h1>
<div class="figure">
<img src="/img/wayback_machine/wayback-machine.jpg" alt="&quot;Success - Batch Archiving external URLs with the Wayback Machine API &amp; R / Rcrawler" />
<p class="caption">"Success - Batch Archiving external URLs with the Wayback Machine API &amp; R / Rcrawler</p>
</div>
<p>Inspired by a session during this week’s <strong>epic</strong> <a href="https://csvconf.com/" target="_blank"><strong>csv,conf v5</strong></a> remote conference event (really, omg, this experience would have been hard to top even pre-Corona), I wanted to <strong>test how easy it would be to batch-archive a set of external links from a single website project with the Wayback Machine API &amp; R</strong>. In her talk on “<a href="https://csvconf.com/speakers/#soila-kenya" target="_blank">Data Archival from a journalist’s perspective</a>”, Soila Kenya discussed the necessity and ways to preserve web content esp. in the case of interactive pieces (i.e. by screen-capturing the user journey in the same manner as Gamers do in Minecraft!).</p>
<p>The web project’s core - GPPI’s rigorous research on Chemical Weapons attacks in Syria - is evidence-heavy (closed and open sources), and while almost all links in the <a href="https://chemicalweapons.gppi.net/data-portal/" target="_blank">CSV dataset</a> have already been archived, most of the external (=outbound) web links weren’t. As websites are constantly re-designed and re-structured, link rot <strong>is</strong> a thing. Isn’t that a <code>purrrfect</code> reason to get started with using the Wayback Machine API?</p>
<p>So what’s to do? As R users we are usually blessed with a legion of user-friendly wrapper packages for most of our use cases. But in the case of Internet Archive’s <a href="https://archive.org/help/wayback_api.php" target="_blank">Wayback Machine API</a> I was a bit surprised. There seems to be only one actively maintained package on CRAN (rOpenSci’s <a href="https://github.com/ropensci/internetarchive" target="_blank"><code>internetarchive</code></a>), but it “just” allows to search and download archived content, not add your own. Short from starting to write my first R package, I tried to figure what’s already possible with the given set of options.</p>
</div>
<div id="theory-crawl---extract---archive" class="section level1">
<h1><span class="header-section-number">2</span> Theory: Crawl - Extract - Archive</h1>
<p>In theory, the approach is a simple 3-step process:</p>
<ol style="list-style-type: decimal">
<li><strong>crawl</strong> all (or a subset of) pages of a single website / web project (let’s focus on HTML content firat; JS reuires more work but there do exist solutions for scraping dynamic / JS-rendered content)</li>
<li><strong>extract</strong> all (or a subset of) external / outbound URLs (http/https)</li>
<li><strong>archive</strong> each external URL with a POST request to the Wayback Machine API</li>
</ol>
<p>Fortunately, <a href="https://twitter.com/fubits/status/1261386300977897472?s=20" target="_blank">#Rstats-Twitter</a> is always there to help out and <a href="https://twitter.com/peterlovesdata/status/1261397322166087682" target="_blank">Peter Meissner</a> hinted at me with Salim Khalil’s <a href="https://github.com/salimk/Rcrawler" target="_blank"><code>RCrawler</code></a> package (cf. <a href="https://www.sciencedirect.com/science/article/pii/S2352711017300110" target="_blank">Khalil/Fakir 2017</a>).</p>
</div>
<div id="minimal-viable-demo---fetchurlstarget-archiveurlsurls" class="section level1">
<h1><span class="header-section-number">3</span> “Minimal Viable Demo” - fetchURLs(target) &amp; archiveURLs(URLs)</h1>
<p><code>Rcrawler</code>’s syntax might seem a bit unorthodox but the package is really feature-rich and we can actually solve Step 1 &amp; 2 with it (and purrr). Then, all we have to do is to figure how to sent a POST request to the Wayback API, and can then work on refining and parallelization.</p>
<div id="setup" class="section level2">
<h2><span class="header-section-number">3.1</span> Setup</h2>
<pre class="r"><code>library(tidyverse)
library(Rcrawler)

# use all CPU cores (~threads) minus 1 to parallelize scraping
cores = (parallel::detectCores() - 1)
print(paste0(&quot;CPU threads available: &quot;, cores))</code></pre>
<pre><code>## [1] &quot;CPU threads available: 7&quot;</code></pre>
</div>
<div id="fetchurlsurl---function-to-crawl-scrape-a-single-website" class="section level2">
<h2><span class="header-section-number">3.2</span> fetchURLs(url) - Function to crawl &amp; scrape a single Website</h2>
<blockquote>
<p>Rcrawler() implicitly returns an <code>INDEX</code> objects to the global environment. <code>INDEX$Url</code> contains a list of URLs vectors.</p>
</blockquote>
<pre class="r"><code>fetchURLs &lt;- function(url, ignoreStrings = NULL) {
  
  # Crawl our target website&#39;s pages
  Rcrawler(
    Website = url,
    no_cores = cores,
    no_conn = 8, # don&#39;t abuse this ;)
    saveOnDisk = FALSE # we don&#39;t the the HTML files of our own website
  )
  
  # extract external URLs from each scraped page
  urls &lt;- purrr::map(INDEX$Url, ~LinkExtractor(.x,
                                                ExternalLInks = TRUE,
                                                removeAllparams = TRUE))
  
  #  only keep external links
  urls_df &lt;- urls %&gt;%
    rvest::pluck(&quot;ExternalLinks&quot;) %&gt;% 
    map_df(tibble)
  
  # helper: optional vector with string/Regex expressions to filter out specific unwanted links
  if (!is.null(ignoreStrings)) {
    urls_df &lt;- urls_df %&gt;% 
      filter(!str_detect(`&lt;chr&gt;`,ignoreStrings))
  }
  
  urls_df &lt;- urls_df %&gt;%
    select(`&lt;chr&gt;`) %&gt;%  # the column which contains the links
    pull() %&gt;% # we just need a single vector of URLs
    unique() %&gt;% # keep only unique
    str_replace(&quot;http:&quot;, &quot;https:&quot;) # make all links https
  
  return(urls_df)
}</code></pre>
</div>
<div id="archiveurl---function-to-archive-a-single-url-with-a-post-request" class="section level2">
<h2><span class="header-section-number">3.3</span> archiveURL() - Function to archive a single URL with a POST request</h2>
<blockquote>
<p>TODO: Check first with test <code>internetarchive</code> package whether a link has already been archived, and archive only if archived link isn’t n days old.</p>
</blockquote>
<pre class="r"><code>archiveURL &lt;- function(target_url) {
  
  # Communiy-built Wayback Machine API / endpoint
  endpoint &lt;- &quot;https://pragma.archivelab.org&quot;
  
  # Alterntive Approach might be not use a quick &amp; dirty GET request
  # https://github.com/motherboardgithub/mass_archive/blob/master/mass_archive.py
  
  # the POST request to the API
  response &lt;- httr::POST(url = endpoint,
                         body = list(url = target_url),
                         encode = &quot;json&quot;)
  
  status_code &lt;- response$status_code
  
  # Only archive if POST request was succesfull
  # TODO: needs more robustness, i.e. some returned paths are not valid
  # Hypothesis: Not valid because URL was archived recently
  # TODO: add check whether URL has been archived already
  
  if (status_code == 200) {
    wayback_path &lt;- httr::content(response)$wayback_id # returns path
    wayback_url &lt;- paste0(&quot;https://web.archive.org&quot;, wayback_path)
    print(paste0(&quot;Success - Archived: &quot;, target_url))
  } else {
    wayback_url &lt;- NULL
    print(paste0(&quot;Error Code: &quot;, status_code, &quot; - couldn&#39;t archive &quot;, target_url))
  }
  
  return(wayback_url)
  
}</code></pre>
</div>
<div id="wrapper-function-to-archive-a-vector-of-urls" class="section level2">
<h2><span class="header-section-number">3.4</span> Wrapper function to archive a vector of URLs:</h2>
<pre class="r"><code>archiveURLs &lt;- function(urls) {
  result &lt;- purrr::map_chr(urls, archiveURL) %&gt;% 
     tibble(wayback_url = .)
  return(result)
  }</code></pre>
</div>
</div>
<div id="proof-of-concept---execution" class="section level1">
<h1><span class="header-section-number">4</span> Proof-of-Concept - Execution</h1>
<div id="scrape-external-links-from-the-project-website" class="section level2">
<h2><span class="header-section-number">4.1</span> Scrape external Links from the project website</h2>
<blockquote>
<p>Demo mode: only first 5 links</p>
</blockquote>
<pre class="r"><code># target Website
url &lt;- &quot;https://chemicalweapons.gppi.net&quot;
unwanted &lt;- c(&quot;gppi|GPPi|dadascience|youtube|Youtube&quot;)

# scrape external Links &amp; filter a particular string from results
urls &lt;- fetchURLs(url, ignoreStrings = unwanted) %&gt;%
  head(5) # Demo to not abuse resources</code></pre>
</div>
<div id="archive-external-links-and-save-as-csv" class="section level2">
<h2><span class="header-section-number">4.2</span> Archive external links and save as CSV</h2>
<blockquote>
<p>Each archiving step naturally takes some time. Making parallel async calls should be the next step.</p>
</blockquote>
<pre class="r"><code>archived_urls &lt;- archiveURLs(urls)

if (!dir.exists(&quot;archived&quot;)) dir.create(&quot;archived&quot;)
write_csv(archived_urls, &quot;archived/archived-urls.csv&quot;)</code></pre>
</div>
</div>
<div id="questionstbd" class="section level1">
<h1><span class="header-section-number">5</span> Questions/TBD</h1>
<ul>
<li>How to parallelize archival (async calls are not supported by httr it seems, only curl?</li>
<li>How to obey quota limitations (with ~setTimeout())?</li>
<li>maybe use <a href="https://ropensci.org/tutorials/internetarchive_tutorial/"><code>internetarchive_tutorial</code></a> to check first whether a URL has already been archived</li>
<li>or use generic API GET requests to retrieve URL, status and ID <a href="">https://archive.readme.io/docs/website-snapshots</a></li>
</ul>
<p>That’s it for today. Hope this is useful for some! If you have a proof-of-concept for making parallel async GET/POST requests, hit me up!</p>
</div>
