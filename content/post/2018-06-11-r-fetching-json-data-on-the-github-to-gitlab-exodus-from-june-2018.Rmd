---
title: '[R] Fetching JSON: Data on the "GitHub to GitLab" Exodus from June 2018'
author: ''
date: '2018-06-11'
slug: r-fetching-json-data-on-the-github-to-gitlab-exodus-from-june-2018
categories:
  - Rstats
  - WebTech
tags:
  - JSON
  - httr
  - jsonlite
lastmod: '2018-06-11T11:26:32+02:00'
output:
  blogdown::html_page:
      number_sections: TRUE
      toc: TRUE
keywords: []
description: "I wanted to find a fast approach for fetching JSON data from a API by grabbing the request URL for GET() directly from a dashboard's DOM. Bonus: Dealing with JSON nested lists in R without going crazy."
thumbnail: "/img/thumbs/gitlab_json_thumb.jpg" # Optional
comment: no
autoCollapseToc: no
postMetaInFooter: no
hiddenFromHomePage: no
contentCopyright: no
reward: no
mathjax: no
mathjaxEnableSingleDollar: no
mathjaxEnableAutoNumber: no
hideHeaderAndFooter: no
flowchartDiagrams:
  enable: no
  options: ''
sequenceDiagrams:
  enable: no
  options: ''
---
**The Problem**

When news broke that [Microsoft acquired GitHub](https://www.theverge.com/2018/6/3/17422752/microsoft-github-acquisition-rumors?utm_campaign=theverge&utm_content=chorus&utm_medium=social&utm_source=twitter) on June 3rd 2018, Twitter was lit. I stumbled upon a Tweet with a link to GitLab's dashboard where you could see various metrics for GitLab's importer API for GitHub:

![*(Screenshot of the GitLab Dashboard for the GitHub Importer on June 4th; [Source](https://monitor.gitlab.net/dashboard/db/github-importer?orgId=1&from=1528041600000&to=now))*](/img/GitLab_JSON/gitlab_dashboard.jpg)

Intrigued by the numbers I immediately wanted to calculate the total of repositories imported from GitHub so far, and I wanted to do this with R. However, it turned out that dealing with JSON is not that easy if you're working with it for the first time, so it took me a couple of [Tweets](https://twitter.com/fubits/status/1003989014037323778) (and days) to finally come up with an automated approach. This is how I proceeded: 

# Inspect a Website's DOM & HTTP Requests

My first idea was to extract the data directly from the according HTML object from the dashboard's [source](https://monitor.gitlab.net/dashboard/db/github-importer?orgId=1&from=1528041600000&to=now). After navigating through the DOM tree I realized that the data is rendered on a `canvas` element and therefore is not part of the syntax.

![*(The dashboard element in the DOM tree. There's no data in the canvas object.)*](/img/GitLab_JSON/gitlab_dom.jpg)

We can see that the dashboard is loading the data dynamically (i.e. you can refresh every item) and there are parameters in the URL. This means that there must be some `API` and `GET/POST` requests in the site's HTML. 

*The formal way would be to consult GitLab's API documentation...*

The quick & dirty way is to locate the request we need **directly in the DOM**. Just right-click somewhere on the page, open the Web Inspector and switch to the `Network Analysis` Tab. There you see all the requests being processed when the dashboard is loaded in your browser (*you might need to reload the page if the tab is empty*).

We're looking for some queries which return data (most probably `XML` or `JSON`), and there you go: The 2nd entry with `type: json` is the request (and response) for the repositories metric:

![*(Screenshot of the Network Analysis Tab in Firefox' DOM inspector)*](/img/GitLab_JSON/gitlab_dom2.jpg)

# Extract JSON


## Quick: JSON from File (copy & paste)
If you're in a hurry, you can copy the JSON data as a string from the response tab, save it as `data.json` and use the magnificent `jsonlite::fromJSON` package to get the data into R.

![*(Direct copy & past access to the JSON data from the query response tab. Fold the values node if you can't access the content field)*](/img/GitLab_JSON/gitlab_json.jpg)

You'll get a list of nested lists^[An analogy I came up with while writing this is think of arriving at the *last* node - but actually you need the *"lastest"*. This is a reference to the German miss-term *Einzigste* which is a superlative of the superlative for "the one and only" which would be something like "the one and *onlyest*".], have to locate the data node and convert the data into something you can work with (i.e. a tidy data frame). This is the level of insanity where I gave up since I couldn't manage to iterate over the list without a for-loop:

![*(\*Help!\*. I can't process the atomic level.)*](/img/GitLab_JSON/gitlab_json_ugly.jpg)

After all, **my goal** was to find a **2-lines solution** in the manners of `readr::read_<data_type>`, which must not resort to any `map_*` / `transpose()` / `DataNinja()` multi-line (and often: single specific use-case) approaches.

```{r echo=FALSE}
blogdown::shortcode('tweet', '1003989014037323778')
```

### It's all about [[1]]

Just in case you eventually end up with a similar problem, here's my (pre-)final 2-lines solution. Addressing the *"lastest"* list of values with `[[1]]` eventually did the trick. Since the data in this JSON is stored as  n+ pairs of lists nested in a single ~~variable~~ list (`values`) I either ended up with a single column with all the values or with two rows of strings. The only working solution until then seemed to be [\@LandonLehman](https://twitter.com/LandonLehman/status/1004055709875654656)'s suggested [double-transpose()](https://twitter.com/LandonLehman/status/1004055709875654656) approach:

```{r echo=FALSE}
blogdown::shortcode('tweet', '1004055709875654656')
```

>Also, MaÃ«lle Salmon was so kind to suggest [jqr](https://twitter.com/ma_salmon/status/1003994439793106944) and [roomba](https://twitter.com/ma_salmon/status/1004042139435749376) which both seem very promising but did not get me to the point where I could turn the nested lists into a data frame. You should definitely check out both and follow [\@ma_salmon](https://twitter.com/ma_salmon).

Only yesterday it somehow striked me that I'm not after the `$values` ~~variable~~ list, but the list's unnamed content `[[1]]`:

![*(Bummer)*](/img/GitLab_JSON/gitlab_json_file.jpg)

### Two Lines, Pt. 1

From here on, it's rather straightforward: 
```{r message=FALSE}
library(tidyverse) # this line doesn't count. 
json_file <- "https://gist.githubusercontent.com/ellocke/498d492dfefd339b8c9884fd07c8f4bb/raw/0efef128bb7dbfe9f3f43c9ce034c6bcdd2ec00a/data.json" # Link to the json.data file in a Gist

# Two lines
json_copy <- jsonlite::fromJSON(json_file)
json_copy$data$result$values[[1]] %>% as.tibble() %>% head(5)
```

>Notice that the values are processed as character strings... We'll deal with this in a second. Same for the V1 variable which actually is date & time in the [Unix Timestamp format aka `POSIX`](https://en.wikipedia.org/wiki/Unix_time).

Ok, that'll basically suffice for a quick analysis, but if you want to *automate* fetching the JSON data from an `API` instead of manually copying & pasting it every time you update your analysis, there's only one way: We need to automate the `GET` request from within `R`.

## Robust: Fetch JSON from API with GET

First, we need the `httr`-package, which is a convenient wrapper for lots of common `HTTP` / `curl` methods. Check out the [vignette for `httr`](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html).

```{r}
library(httr)
```

Next, we need the query as an `URL` which we can grab from the Network Analysis tab (see above).

```{r}
DOM_URL <- parse_url('https://monitor.gitlab.net/api/datasources/proxy/2/api/v1/query_range?query=sum(rate(github_importer_imported_repositories{environment = "prd"}[9000s])) * 3600&start=1528041600&end=1528698457&step=9000') # we either need to escape "prd" with \"prd\" or use ' ' for the outer string wrapping

json_raw <- GET(url = DOM_URL)
http_status(json_raw)$message # let's check if the GET request was a success (=200)
# content(json_dom, "parsed") # one way to see the content
```

## Parse JSON

Now we need to parse the content we received from the request to something we can further process. Right now, the `$content` node is literally raw:
```{r}
# json_raw$content
```

![*(Rawrrrrrr)*](/img/GitLab_JSON/gitlab_json_raw.jpg)

Let's parse it. `httr` offers a couple of convenient shortcuts here. We can parse the JSON as a `String` (with `as = "text"`) and then process it with `jsonlite::fromJSON`. ~~Or we can instantly parse it to "parsed"" JSON with 1 line.~~ (Not working properly for me, yet)

```{r}
# as = "text"
json_parsed <- content(json_raw, as = "text") # simple, for follow-up with jsonlite::fromJSON()
# str(json_parsed)

#  as = "parsed" 
# json_object <- content(json_raw, as = "parsed", type = "application/json", encoding = "UTF-8")
## this forces us to use something like
## json_object$data$result[[1]]$values %>%
##    transpose() %>%
##      {do.call(cbind,.)} %>%
##      as.tibble() %>%
##      unnest()
## OR
## json_object$data$result[[1]]$values %>%
##      transpose() %>%
##      set_names(c("date", "value")) %>%
##      as.tibble() %>%
##      unnest()
## HORRIBLE!
```

# From JSON to Tidy

Now - either on the JSON string or the JSON object (list of lists) - we just need to `%>% as.tibble()`

```{r}
json_object <- jsonlite::fromJSON(json_parsed) # (if parsed with 'as = "text"')
json_object$data$result$values[[1]] %>% as.tibble() -> json_dirty
# json_object$data$result[[1]][[2]] %>% as.tibble() -> json_dirty # not working, yet
# json_object[["data"]][["result"]][[1]][["values"]] %>% as.tibble() # not working, yet
head(json_dirty, 5) #> 2x chr
```

As both columns/variables are coded as character strings - and we want to have the first column as a `Date` type variable - we'd need to use `mutate` on both. Furthermore, `lubridate::as_datetime` is of insane value here but needs something numeric, too. We actually have Unix Timestamp data here which would be `something * something * 86400` seconds if you wanted to convert it to a human readable format. `as_datetime()` simply does it with one call.

## Tidy Date \#1 with lubridate

```{r}
json_dirty %>%
  mutate(V1 = lubridate::as_datetime(as.numeric(V1)), # first to int, then to date
         V2 = as.numeric(V2)) -> json_tidy
head(json_tidy)
```

However, we can simplify this **even further**!

## Tidy Date #2: type_convert() to the Rescue / Two Lines, Pt. 2

```{r}
# json_object <- jsonlite::fromJSON(json_parsed)
json_object$data$result$values[[1]] %>% as.tibble() %>% type_convert() -> json_dirty
head(json_dirty, 5) # > int & dbl
```

Now the date & time / `POSIX` column:

```{r}
json_dirty %>%
  mutate(V1 = lubridate::as_datetime((V1))) -> json_tidy
head(json_tidy) # > V1 == <dttm>
```

**Ok, almost done. Let's rename the variables to something meaningful with `rename()` and finally plot the data**

(Yes, we could also have used

  > `json_tidy %>% mutate(date = V1, repos = V2) %>% select(-V1, -V2)`

but `rename()` seems more efficient here.

```{r}
json_tidy %>% rename(date = V1, repos = V2) -> json_tidy
```


# Plots
## Hourly # of Repos migrated from GitHub to GitLab
```{r}
json_tidy %>%
  ggplot(aes(x = date, y = repos)) +
  geom_line()
```

## Total # of Repos, cumulated over Time
```{r}
json_tidy %>% 
  ggplot(aes(x = date, y = repos)) + 
    geom_line(aes(y = cumsum(repos)))
```

## Bonus: GitHub API Rate Limit Hits

Same procedure as above:

  + find the request's URL
  + fetch with `httr`, parse with `jsonlite`
  + tidy with `dplyr` & plot with `ggplot`
    
**GET the Query**

```{r}
DOM_URL_rates <- parse_url('https://monitor.gitlab.net/api/datasources/proxy/2/api/v1/query_range?query=sum(rate(github_importer_rate_limit_hits%7Benvironment%3D%22prd%22%7D%5B3600s%5D))%20*%203600&start=1528041600&end=1528698457&step=3600') # we either need to escape "prd" with \"prd\" or use ' ' for the outer string wrapping
json_rates <- GET(url = DOM_URL_rates)
# http_status(json_rates)
# content(json_dom, "parsed")
```

**Parse JSON**

Now that we know how deal with the JSON output we can actually do the processing in a single working step. 

> Technically, without intending this would fit in two lines of code :)

```{r}
json_rates <- content(json_rates, "text", encoding = "UTF-8") # UTF-8 == default
jsonlite::fromJSON(json_rates)$data$result$values[[1]] %>%
  as.tibble() %>% 
  type_convert() %>%
  mutate(V1 = lubridate::as_datetime(V1)) %>% 
  rename(date = V1, limit_hits = V2) -> json_rates
```

## Plot # of Repos & GitHub API Rate Limit Hits
```{r}
ggplot() +
  geom_line(data = json_tidy, aes(x = date, y = repos), color = "blue") +
  geom_line(data = json_rates, aes(x = date, y = limit_hits), color = "red")
```

> Done! Actually, not that hard right? Only took me a couple of days to figure this out... I'll update this post as soon as I have found a more efficient way (in lines of code).
